到目前为止，我们的大多数示例都是关于批处理作业的。 正如我们已经看到的，要使批处理作业适应故障，我们需要确保将工作分散成小块并动态分配给Worker。 Google的规范框架是MapReduce [9]，随后被Flume [10]取代。

从很多方面来说，服务工作比批处理工作更自然地适合于抗故障能力。 他们的工作自然会分成小块（单个用户请求），然后动态分配给Workers。自从服务互联网流量开始以来，就一直采用通过在服务器群集之间进行负载平衡来处理大量请求的策略。

然而，还有多个服务应用程序并不自然地适合这种模式。典型的例子是你直观地描述为某个特定系统的领导者的任何服务器。这样的服务器通常会维护系统的状态(在内存中或其本地文件系统上)，如果运行它的机器宕机，新创建的实例通常将无法重新创建系统状态。 另一个例子是，当你要处理大量数据（超出一台计算机的容量）时，你决定在100台服务器（分别容纳1％的数据）之间分片数据，并处理该部分的请求的数据。 这类似于将工作静态分配给批处理工作人员。 如果其中一台服务器出现故障，你（暂时）将失去为部分数据提供服务的能力。最后一个例子是，系统中的其他部分是否可以通过主机名感知到服务器。 在这种情况下，无论服务器的结构如何，如果此特定主机失去网络连接，系统的其他部分都将无法与它联系 [11]。

### 管理状态

正如前面描述的，管理状态一个共同主题是，在尝试像cattle [12]一样的工作时，状态是问题的根源。每当您替换其中的一个牛工作时，您都会失去所有过程中的状态（以及以前的所有状态）。（如果作业已移至另一台计算机上）。 这意味着正在处理的状态应该被视为暂时的，而“实际存储”需要出现在其他地方。

解决此问题的最简单方法是将所有存储提取到外部存储系统。这意味着任何超出了单个服务请求范围（在服务作业情况下）或处理一个数据块（在批处理情况下）的范围的东西都需要在机器外持久存储中存储。 如果所有的本地状态都是不可变的，那么应用程序具有抗故障能力应该相对轻松一些。

不幸的是，大多数应用程序并不是那么简单。 可能自然想到的一个问题是：“这些持久的存储解决方案是如何实现的？它们可是cattle？ 答案应该是“是”。 持久状态可以由cattle通过状态复制来管理。 在不同的层面上，RAID阵列是一个类似的概念； 我们将磁盘视为瞬态磁盘（接受其中一个磁盘可能已消失的事实），同时仍保持状态。 在服务器世界中，这可以通过保存单个数据的多个副本并进行同步以确保每个数据被复制足够的次数（通常为3到5次）来实现。 请注意，正确设置此设置非常困难（需要某种共识处理方式来处理写操作），因此Google开发了许多专用存储解决方案 [13]，这些存储解决方案使大多数应用程序都采用了所有状态都是瞬态的模型。

Cattle可以使用的其他类型的本地存储包括本地保存的“可重新创建”数据，以改善服务延迟。 缓存是这里最明显的例子：缓存不过是将临时状态存储在临时位置的临时本地存储而已，但该状态的存储库不会一直消失，因此平均而言可以获得更好的性能特征。 Google生产基础架构的主要工作是调配缓存以满足延迟目标，并调配核心应用程序以实现总负载。 这使我们避免了缓存层丢失时的中断，这是因为未缓存的路径已调配来处理总负载（尽管具有更高的延迟）。 但是，这里有一个明显的权衡：在缓存容量丢失时，要在冗余上花多少钱来减轻中断的风险。

与缓存类似，可以在应用程序预热时将数据从外部存储拉到本地，以改善请求服务的等待时间。

在写入的数据多于读取的情况下，使用本地存储的另一种情况是批量写入。 这是监控数据的常用策略（例如，考虑从车队中收集CPU利用率统计信息以指导自动缩放系统），但是可以在部分数据会消失的可接受位置使用它， 要么是因为我们不需要100％的数据覆盖率（这是监控场景），要么是因为可以重新创建消失的数据（这是批处理作业的情况，该批处理作业以块为单位处理数据，并为每个数据写入一些输出块）。 请注意，在许多情况下，即使特定的计算需要花费很长时间，也可以通过将状态周期性地指向持久性存储的检查点，将其划分为较小的时间窗口。

### 连接到一个服务

如前所述，如果系统中的任何内容都以硬编码形式（或在启动时作为配置参数提供）在其上运行程序的主机名，则你的程序副本就不会存在。 但是，要连接到你的应用程序，另一个应用程序确实需要从某个地方获取你的地址。 在哪里？

答案是要有一个额外的中间层。 也就是说，其他应用程序通过某些标识符引用你的应用程序，该标识符在特定的“后端”实例的重新启动期间是持久的。 该标识符可以由计划程序将应用程序放置在特定计算机上时写入的另一个系统来解析。 现在，为了避免在对应用程序发出请求的关键路径上进行分布式存储查找，客户端可能会在启动时查找可以找到你的应用程序的地址，并建立连接，并在后台对其进行监视 。 这通常称为服务发现，许多计算产品都具有内置或模块化解决方案。 大多数此类解决方案还包括某种形式的负载平衡，从而进一步减少了与特定后端的耦合。

此模型的后果是，在某些情况下，你可能需要重复请求，因为与您通信的服务器可能在管理应答之前就已被关闭[14]。重试请求是网络通信（例如，移动应用程序到一个服务器）的标准做法，但是对于诸如服务器与其数据库进行通信之类的事情可能不太直观。 因此，以一种能够妥善处理此类故障的方式来设计服务器的API至关重要。 对于变异请求，处理重复的请求非常棘手。 你要保证的属性是幂等的某种变体，即两次发出请求的结果与一次发出请求的结果相同。 客户端分配的标识符是帮助实现幂等性的一种有用工具：如果你要创建商品（例如，将比萨饼递送到特定地址的订单），则客户端会为该订单分配一些标识符； 并且如果已经记录了带有该标识符的订单，则服务器会认为它是重复请求并报告成功（它也可能会验证该订单的参数是否匹配）。

另一件令人惊讶的事情是，有时调度程序由于某些网络问题而失去与特定计算机的联系。 然后，它确定所有工作都丢失了，并将其重新安排到其他机器上，然后机器又回来了！ 现在，我们在两台不同的计算机上有两个程序，都认为它们是“ replica072”。 它们消除歧义的方法是检查地址解析系统引用了其中一个（另一个应该终止它自己或被终止）。 但对于幂等也有另外一种情况：执行相同工作并发挥相同作用的两个副本是请求重复的另一个潜在来源。

### 一次性代码

先前的大多数讨论都集中在生产质量的作业上，这些作业要么是为用户流量提供服务，要么是生产生产数据的数据处理管道。 但是，软件工程师的生命还涉及运行一次性分析，探索性原型，自定义数据处理管道等。 这些需要计算资源。

通常，工程师的工作站是满足计算资源需求的令人满意的解决方案。 例如，如果要对服务在过去一天产生的1GB日志进行自动浏览以检查是否在错误行B之前总是出现可疑行A，则他们可以下载日志，编写简短的Python 脚本，并运行一两分钟。

但是，如果他们想通过去年生成的1 TB日志（出于类似目的）自动进行略读，那么大约等待一天的时间才能收到结果，这可能是不可接受的。 允许工程师在几分钟内（使用数百个内核）在分布式环境上运行分析的计算服务意味着现在进行分析与明天进行分析之间的区别。 对于需要迭代的任务（例如，如果我需要在查看结果之后进行查询优化），区别可能在于一天完成一次操作与根本不执行。

这种方法有时会引起的一个问题是，允许工程师仅在分布式环境上运行一次性工作可能会浪费资源。 当然，这是一个折衷，但是应该自觉地做出。 工程师运行的处理成本不会比工程师在编写处理代码上花费的时间昂贵得多。 确切的权衡值取决于组织的计算环境及其向工程师支付的费用，但是，千核每小时花费的费用几乎不可能花费一天的工程工作。 在这方面，计算资源类似于标记，我们在本书的开头讨论了这些标记。 对于公司而言，建立一个流程以获取更多的计算资源的机会很少，但是此流程所浪费的工程机会和时间可能要比节省的成本高得多。

也就是说，计算资源与标记不同，因为很容易意外地占用过多资源。 尽管不太可能有人会拿出一千个标记，但完全有可能有人会不经意间写出一个占用一千台机器的程序 [15]。自然的解决方案是为每个工程师设置资源使用配额。 Google使用的一种替代方法是观察到，因为我们免费有效地运行了低优先级批处理工作负载（请参阅稍后的多租户部分），所以我们可以为工程师提供几乎无限量的低优先级批处理工作配额，这已经足够了，适用于大多数一次性工程任务。

## 随着时间和规模的Caas

我们在上面讨论了CaaS在Google上的发展历程以及实现这一目标所需的基本部分-“仅仅给我资源就能运行我的任务”这一简单的任务是如何转化为像Borg这样的实际架构的。 CaaS体系结构如何影响软件在整个时间和规模上的生命周期的几个方面值得仔细研究。

### 容器作为一个抽象

正如我们之前所描述的，容器主要表示为一种隔离机制，这是一种实现多租户的方法，同时将共享一台机器的不同任务之间的干扰降至最低。 这是最初的动机，至少在Google中是这样。 但是，事实证明，容器在抽象化计算环境方面也起着非常重要的作用。

容器提供了已部署软件和运行它的实际计算机之间的抽象边界。 这意味着随着时间的推移，机器会发生变化，只有容器软件（大概由一个团队来管理）才需要进行调整，而应用软件（随组织的发展由每个团队来管理）可以保持不变。

让我们讨论容器化抽象如何允许组织来管理变更的两个示例。

文件系统抽象提供了一种无需管理自定义机器配置即可合并公司未编写的软件的方法。 这可能是组织在其数据中心中运行的开源软件，也可能是组织希望在其CaaS上使用的收购软件。 如果没有文件系统抽象，则在二进制文件上加载期望不同的文件系统布局（例如，在/bin/foo/bar中使用辅助二进制文件）将需要修改队列中所有机器的基本布局，或者对队列进行分段或修改软件（出于许可考虑，这可能很困难，甚至可能无法实现）。

即使这些解决方案可能是可行的，如果导入一个外部软件是在其生命周期内只发生一次的事情，但是如果导入软件成为一种常见的（甚至是很少见的）实践，那也不是一个可持续的解决方案。

某种文件系统抽象也有助于依赖管理，因为它允许软件预先声明和预打包软件需要运行的依赖关系（例如，特定版本的库）。 根据机器上安装的软件的不同，可能会导致泄漏的抽象，迫使每个人都使用相同版本的预编译库，并且即使不是不可能，也很难升级任何组件。

容器还提供了一种简单的方法来管理计算机上的命名资源。典范的例子是网络端口。 其他命名资源包括专门的目标； 例如GPU和其他加速器。

Google最初并未将网络端口作为容器抽象的一部分，因此二进制文件必须自行搜索未使用的端口。 结果，PickUnusedPortOrDie函数在Google C ++代码库中的使用量超过20,000。Docker是在引入Linux名称空间之后构建的，它使用名称空间为容器提供虚拟专用NIC，这意味着应用程序可以在所需的任何端口上进行侦听。 然后，Docker网络堆栈将机器上的端口映射到容器内端口。 Kubernetes最初是建立在Docker之上的，再往前走了一步，并要求网络实现将容器（以Kubernetes的说法是“ pod”）视为可从主机网络获得的“真实” IP地址。 现在，每个应用程序都可以在他们想要的任何端口上进行监听，而不必担心发生冲突。

当处理未设计在特定计算堆栈上运行的软件时，这些改进特别重要。 尽管许多流行的开源程序都有要使用哪个端口的配置参数，但是在如何配置此端口之间并没有一致性。

#### 容器和隐式依赖

与任何抽象一样，Hyrum的隐式依赖定律也适用于容器抽象。 它可能比平常应用得更多，这是因为用户数量众多（在Google，所有生产软件以及许多其他软件都将在Borg上运行），还因为用户在使用文件系统之类的东西时不会觉得自己在使用API。 （甚至很少考虑此API是否稳定，版本等）

为了说明这一点，让我们回到Borg在2011年经历的进程ID空间耗尽的示例。您可能想知道为什么进程ID可以耗尽。 它们不只是可以从32位或64位空间分配的整数ID吗？ 在Linux中，实际上在[0，...，PID_MAX-1]范围内分配它们，其中PID_MAX默认为32,000。 但是，可以通过简单的配置更改（较大的限制）来提高PID_MAX。 问题解决了？

好吧，不。 根据Hyrum的定律，在Borg上运行的进程的PID限制在0 ... 32,000范围内这一事实成为人们开始依赖的隐式API保证； 例如，日志存储过程取决于PID可以以五位数字存储的事实，并且因为记录名称超过了允许的最大长度而中断了六位数的PID。 解决该问题成为了一个漫长的，分为两个阶段的项目。 首先，临时限制单个容器可以使用的PID数量（这样，单个泄漏线程的工作就不会使整个计算机无法使用）。 其次，为线程和进程分配PID空间。（因为事实证明，很少有用户依赖于分配给线程的PID的32,000保证，而不是依赖于进程。因此，我们可以增加线程的限制并将其保持在32,000的进程。）第三阶段是引入PID Borg的名称空间，为每个容器提供自己的完整PID空间。 可以预见的（再次是希鲁姆定律），假设三元组{hostname，timestamp，pid}唯一地标识一个进程，如果引入了PID名称空间，这将破坏很多系统。 八年后，仍在努力确定所有这些位置并进行修复（并向后移植任何相关数据）。

这里的重点不是您应该在PID名称空间中运行容器。尽管这是个好主意，但这不是有趣的课程。 建造Borg的容器时，PID名称空间不存在。 即使他们做到了，也不能指望2003年设计Borg的工程师意识到引入它们的价值。甚至到现在，一台机器上肯定还有一些资源没有被充分隔离，这有一天可能会引起问题。 这突显了设计一个容器系统的挑战，该容器系统将随着时间的推移而被证明是可维护的，因此，使用由更广泛的社区开发和使用的容器系统的价值也很重要，在这些社区中，其他人已经遇到了这些类型的问题，并且已经吸取了教训。

### 一劳永逸

如前所述，原始的WorkQueue设计仅针对某些批处理作业，最终所有工作共享一个由WorkQueue管理的服务器池，并且为服务提供了不同的体系结构，每个特定的服务任务均在其自己的位置运行， 专用服务器池。 对于每种工作负载，等效的开源程序将运行一个单独的Kubernetes集群（为所有批处理作业添加一个池）。

Borg项目于2003年启动，旨在（最终成功地）构建一种将这些分散的池合并为一个大池的计算服务。 Borg的池涵盖了服务和批处理作业，并成为任何数据中心中的唯一池（等效于为每个地理位置的所有工作负载运行一个大型Kubernetes集群）。 这里有两个重要的效率提升值得讨论。

第一个是服务的机器变成了cattle（Borg设计文档所说的：“机器是匿名的：只要具有正确的特性，程序就不在乎它们在哪台机器上运行”）。 如果每个管理服务工作的团队必须管理自己的计算机池（他们自己的集群），那么维护和管理该池的组织开销将被应用到这些团队中的每个团队。 随着时间的流逝，这些池的管理实践将随着时间的推移而发生变化，从而使公司范围内的更改（例如迁移到新的服务器体系结构或切换数据中心）变得越来越复杂。 统一的管理基础架构（即针对组织中所有工作负载的通用计算服务）使Google避免了这种线性扩展因子。 机队中的物理机没有不同的管理方法，只有Borg [16]。

第二个比较微妙，可能不适用于每个组织，但与Google息息相关。 批处理和服务工作的独特需求被证明是相辅相成的。 服务作业通常需要超额配置，因为它们需要有能力为用户流量提供服务，而不会显着降低延迟，即使在使用高峰或部分基础架构中断的情况下也是如此。 这意味着仅运行正在服务的作业的计算机将无法充分利用。 尝试通过过度使用机器来利用该空闲时间是很诱人的，但这首先就破坏了该空闲时间的目的，因为如果发生尖峰/中断，我们将无法获得所需的资源。

然而，这个推理只适用于提供作业! 如果我们在一台机器上有许多服务作业，这些作业请求的RAM和CPU之和等于机器的总大小，那么就不能再在机器上放置更多的服务作业，即使资源的实际利用率仅为容量的30%。但我们可以(在Borg,会把批处理作业的剩余70%,如果任何的服务工作需要内存或CPU,我们将收回它的批处理作业(通过冻结他们的CPU或杀死的RAM)。因为批处理作业对吞吐量感兴趣(以跨越数百个Worker的总量来衡量，而不是针对单个任务)，而且它们的单个副本无论如何都是cattle，它们将非常乐意吸收服务于这些作业的空闲能力。

根据给定计算机池中工作负载的形状，这意味着要么所有批处理工作负载都有效地在空闲资源上运行（因为无论如何我们都是在服务作业的空闲状态下为它们付费），或者所有正在处理的作业负载都是有效地仅为他们使用的资源付费，而不是为他们的抗故障能力所需的松弛容量付费（因为批处理作业正在该松弛空间中运行）。 在大多数情况下，对于Google而言，事实证明我们可以免费有效地批量运行。

#### 多租户服务作业

早先，我们讨论了计算服务必须满足的一些要求才能适合运行服务作业。 如前所述，由通用计算解决方案管理服务作业有多个优点，但这也带来了挑战。 一个值得重复的特殊要求是发现服务，在第528页上的“连接到服务”中进行了讨论。例如，当我们想将托管计算解决方案的范围扩展到服务任务时，还有许多其他新要求：

- 需要限制对作业的重新安排：虽然可以终止并重新启动批处理作业的50％副本是可行的（因为这只会在处理过程中造成短暂的故障，而我们真正关心的是吞吐量），但这不太可能被接受杀死并重新启动服务作业的50％副本（因为剩余的作业可能太少，无法在等待重新启动的作业再次出现时为用户流量提供服务）。
- 批处理作业通常可以在没有警告的情况下被杀死。 我们丢失的是一些已经执行的处理，可以重做。 当服务作业被毫无预兆地杀死时，我们可能会冒一些面向用户的流量返回错误或（最好）增加延迟的风险。 最好提前几秒钟发出警告，以便作业可以完成正在处理的请求，而不接受新请求。

由于上述效率原因，Borg涵盖了批处理作业和服务作业，但是多种计算产品将这两个概念分开——通常是，用于批处理作业的共享计算机池，以及用于服务于作业的专用，稳定的计算机池。但是，不管两种类型的工作是否使用相同的计算体系结构，这两组作业都可以像对待cattle一样受益。

### 提交的配置

Borg调度程序接收要在单元中运行的复制服务或批处理作业的配置，作为远程过程调用（RPC）的内容。 服务运营商可以使用命令行界面（CLI）来管理该服务，该命令行界面发送这些RPC，并将CLI的参数存储在共享文档中或在其头部中。

通常，依靠文档和部落知识而不是提交给存储库的代码很少是一个好主意，因为文档和部落知识都倾向于随着时间的流逝而恶化（请参阅第3章）。 但是，演进的下一个自然步骤-将CLI的执行包装在本地开发的脚本中-仍然不如使用专用的配置语言来指定服务的配置。

随着时间的流逝，逻辑服务的运行时状态通常会超出一个数据中心中跨多个轴的一组复制容器的范围：

- 将它的存在分布在多个数据中心上（既用于用户亲和力又用于抗故障能力）。
- 除了生产环境/配置之外，它还将具有暂存和开发环境。
- 它将以附加服务的形式累积其他不同类型的其他复制容器，例如服务随附的内存缓存。

如果可以使用标准化配置语言来表达此复杂的设置，从而简化了标准操作（例如“将我的服务更新为二进制的新版本，但占用的容量不超过5％”，则可以简化服务的管理） 在任何给定时间”）。

标准化的配置语言提供了标准的配置，其他团队可以轻松地将其包含在服务定义中。 与往常一样，我们强调这种标准配置在时间和规模上的价值。 如果每个团队都编写不同的自定义代码片段来支持其Memcached服务，则很难执行组织范围内的任务，例如换用新的Memcache实现（例如出于性能或许可原因）或推送安全更新。 所有的Memcache部署。 还要注意，这种标准化的配置语言是部署自动化的必要条件（请参阅第24章）。

## 选择一个计算服务

任何组织都不太可能重蹈谷歌的覆辙，从零开始构建自己的计算体系结构。现在，现代计算产品既可以在开源世界中使用(比如Kubernetes或Mesos，或者在不同的抽象级别上使用OpenWhisk或Knative)，也可以作为公共云管理产品使用(同样，在不同的复杂性级别上，从谷歌云平台的管理实例组或亚马逊Web服务弹性计算云[Amazon EC2]自动伸缩;到类似Borg的托管容器，如微软Azure Kubernetes Service [AKS]或谷歌Kubernetes Engine [GKE];到AWS Lambda或谷歌的Cloud Functions等无服务器产品)。

但是，大多数组织都会选择计算服务，就像Google在内部所做的一样。请注意，计算基础架构具有较高的锁定因子。 原因之一是因为将以一种利用系统所有特性的方式来编写代码（希鲁姆定律）； 因此，例如，如果您选择基于VM的产品，则团队将调整其特定的VM映像； 并且如果您选择特定的基于容器的解决方案，团队将调出集群管理器的API。 如果您的体系结构允许代码将虚拟机（或容器）当作宠物对待，那么团队将这样做，然后很难找到依赖于将它们像牛（甚至不同形式的宠物）一样对待的解决方案。

要显示甚至如何锁定计算解决方案的最小细节，请考虑Borg如何运行用户在配置中提供的命令。 在大多数情况下，该命令将执行二进制文件（可能后跟多个参数）。 但是，为了方便起见，Borg的作者还包括了传递shell脚本的可能性； 例如，while true；do ./my_binary; done [17]，尽管二进制执行可以通过简单的fork-and-exec（Borg所做的）来完成，但是shell脚本需要由像Bash这样的shell运行。 因此，Borg实际上执行了/ usr / bin / bash -c $ USER_COMMAND，它在简单的二进制执行中也起作用。

你可能会认为这不是一个有风险的改变;毕竟，我们控制着环境，我们知道这两个二进制文件都存在，所以这应该不会不起作用。实际上，这并不奏效的原因是Borg工程师并不是第一个注意到运行Bash的额外内存开销的人。一些团队很有创意，他们希望限制内存的使用，并用一段自定义编写的“执行第二个参数”代码替换(在他们的自定义文件系统覆盖层中)Bash命令。当然,这些团队非常清楚自己的内存使用,所以当Borg团队改变了过程运动员使用灰(不覆盖的自定义代码),他们的内存使用增加(因为它开始包括灰使用而不是自定义代码的使用),这引起警报,回滚更改,还有一定程度的不愉快。

计算服务选择难以随时间更改的另一个原因是，任何计算服务选择最终都会被庞大的帮助服务生态系统所包围，这些生态系统包括日志记录，监视，调试，警报，可视化，实时分析， 配置语言和元语言，用户界面等。 这些工具将需要作为计算服务更改的一部分进行重写，即使对于中型或大型组织来说，即使理解和枚举这些工具也可能是一个挑战。

因此，计算体系结构的选择很重要。 与大多数软件工程选择一样，这需要权衡取舍。 让我们讨论一些。

### 集中化与定制

从计算堆栈的管理开销的角度（以及从资源效率的角度），组织可以做的最好的事情就是采用单个CaaS解决方案来管理其整个机器群，并为每个人仅使用可用的工具。 这确保了随着组织的发展，管理团队的成本仍然可控。 这条路径基本上就是Google对Borg所做的。